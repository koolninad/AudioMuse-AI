# AudioMuse-AI Deployment Configuration
#
# WORKER TEMPLATE - Run this for heavy CPU/GPU tasks like analysis and clustering, connected to a lightweight server with Jellyfin and AudioMuse-AI Flask application and databases.
# This configuration is intended for deployment on a server with NVIDIA GPU support.
#

version: '3.8'
services:
  # AudioMuse-AI Worker service (GPU-dependent)
  audiomuse-ai-worker:
    image: ghcr.io/neptunehub/audiomuse-ai:latest-nvidia
    container_name: audiomuse-ai-worker-instance
    ports:
      - "8029:8000"  # Expose worker API
    environment:
      SERVICE_TYPE: "worker"
      MEDIASERVER_TYPE: "jellyfin"
      JELLYFIN_USER_ID: "${JELLYFIN_USER_ID}"
      JELLYFIN_TOKEN: "${JELLYFIN_TOKEN}"
      JELLYFIN_URL: "${JELLYFIN_URL}"
      POSTGRES_USER: ${POSTGRES_USER:-audiomuse}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-audiomusepassword}
      POSTGRES_DB: ${POSTGRES_DB:-audiomusedb}
      POSTGRES_HOST: "${WORKER_POSTGRES_HOST:-postgres}" # Replace via WORKER_POSTGRES_HOST in .env when running remotely
      POSTGRES_PORT: "${POSTGRES_PORT:-5432}"
      REDIS_URL: "${WORKER_REDIS_URL:-redis://redis:6379/0}"  # Set WORKER_REDIS_URL in .env for remote connections
      AI_MODEL_PROVIDER: "${AI_MODEL_PROVIDER}"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
      OPENAI_SERVER_URL: "${OPENAI_SERVER_URL}"
      OPENAI_MODEL_NAME: "${OPENAI_MODEL_NAME}"
      GEMINI_API_KEY: "${GEMINI_API_KEY}"
      MISTRAL_API_KEY: "${MISTRAL_API_KEY}"
      TEMP_DIR: "/app/temp_audio"
      NVIDIA_VISIBLE_DEVICES: "0"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      USE_GPU_CLUSTERING: "${USE_GPU_CLUSTERING:-true}"
    volumes:
      - temp-audio-worker:/app/temp_audio
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
volumes:
  temp-audio-worker: