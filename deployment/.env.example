# Copy this file to `.env` and fill in the values that match your setup.
# Docker Compose files under deployment/ read these variables to keep settings in one place.
#
# IMPORTANT: 
# 1. This file must be named exactly ".env" (not .env.txt or .env.example)
# 2. It must be in the SAME directory as your docker-compose-*.yaml file
# 3. Do NOT use spaces around the = sign
# 4. Do NOT use quotes around values (unless required by the value itself)
# 5. After editing, restart containers: docker-compose down && docker-compose up -d
#
# SPECIAL CHARACTERS IN VALUES:
# If your password or API key contains special characters like: $ ` " ' \ # ! & * ( ) [ ] { } | ; < > ?
# you may need to:
#   - Avoid quotes entirely: GEMINI_API_KEY=AIza$pecial!Key  (usually works)
#   - OR use single quotes if the value has $: GEMINI_API_KEY='AIza$pecial!Key'
#   - OR escape with backslash: GEMINI_API_KEY=AIza\$pecial\!Key
# Most problematic characters: $ (variable expansion), ` (command substitution), " (string delimiter)
#
# TROUBLESHOOTING:
# If API keys don't work, verify:
#   - File is named ".env" exactly (check with: ls -la)
#   - No spaces: GEMINI_API_KEY=AIza... (not GEMINI_API_KEY = "AIza...")
#   - No unescaped special characters (especially $ ` " ')
#   - Restart containers after changing this file
# If all else fails, try hardcoding the value directly in docker-compose-*.yaml to isolate the issue

# --- Jellyfin ---
JELLYFIN_USER_ID=YOUR_JELLYFIN_USER_ID
JELLYFIN_TOKEN=YOUR_JELLYFIN_API_TOKEN
JELLYFIN_URL=http://jellyfin.example.com:8096

# --- Emby ---
EMBY_USER_ID=
EMBY_TOKEN=
EMBY_URL=

# --- Navidrome ---
NAVIDROME_URL=
NAVIDROME_USER=
NAVIDROME_PASSWORD=

# --- Lyrion ---
LYRION_URL=http://lyrion.example.com

# --- Shared backend configuration ---
POSTGRES_USER=audiomuse
POSTGRES_PASSWORD=audiomusepassword
POSTGRES_DB=audiomusedb
POSTGRES_PORT=5432
POSTGRES_HOST=postgres
REDIS_URL=redis://redis:6379/0

# --- Remote worker integration ---
WORKER_URL=http://worker.example.com:8029/worker
WORKER_POSTGRES_HOST=server.example.com
WORKER_REDIS_URL=redis://server.example.com:6379/0

# --- AI Model Configuration ---
# Choose your AI provider: NONE, OLLAMA, OPENAI, GEMINI, OPENAI, or MISTRAL
AI_MODEL_PROVIDER=NONE

# --- OpenAI / OpenRouter Configuration ---
# For OpenRouter (https://openrouter.ai):
#   1. Get API key from https://openrouter.ai/keys
#   2. Set OPENAI_API_KEY to your OpenRouter API key
#   3. Set OPENAI_SERVER_URL to https://openrouter.ai/api/v1/chat/completions
#   4. Set OPENAI_MODEL_NAME to your desired model (e.g., openai/gpt-4, meta-llama/llama-3.2-3b-instruct:free)
# For OpenAI directly: Use https://api.openai.com/v1/chat/completions and your OpenAI API key
OPENAI_API_KEY=
OPENAI_SERVER_URL=https://openrouter.ai/api/v1/chat/completions
OPENAI_MODEL_NAME=
# Optional: Delay between API calls to respect rate limits (default: 7 seconds)
OPENAI_API_CALL_DELAY_SECONDS=7

# --- Other AI Provider API Keys ---
GEMINI_API_KEY=
MISTRAL_API_KEY=

# --- GPU Acceleration for Clustering ---
# Enable GPU-accelerated clustering using RAPIDS cuML (requires NVIDIA GPU)
# Set to true to use GPU for KMeans, DBSCAN, and PCA in clustering tasks
# Automatically falls back to CPU if GPU is unavailable
# Default: false (CPU only)
USE_GPU_CLUSTERING=false
